import torch
import multiprocessing as mp
import os
from datasets import load_dataset, concatenate_datasets
from typing import List
from utils.config_manager import ConfigManager, DatasetConfig
from utils.dataset_processor import DatasetProcessor
from utils.audio_worker import worker_process, AudioWorker
from utils.reader_worker import reader_worker_process


class PipelineManager:
    """Manages the entire audio processing pipeline"""

    def __init__(self, config_path: str = "config.yaml"):
        self.config_manager = ConfigManager(config_path)
        self.base_settings = self.config_manager.get_base_settings()
        self.save_settings = self.config_manager.get_save_settings()
        self.sample_rate = self.config_manager.get_sample_rate()
        self.num_layers = self.config_manager.get_num_layers()
        self.num_gpus = torch.cuda.device_count()

        os.makedirs(self.base_settings.OUT_DIR, exist_ok=True)

    def validate(self):
        """Validate configuration and environment"""
        print("ğŸ” Validating configuration...")
        self.config_manager.validate_datasets()

        if self.num_gpus == 0:
            raise RuntimeError("âŒ ERROR: No CUDA devices found!")

        print(f"âœ… Found {self.num_gpus} GPU(s)")
        print(f"âœ… Sample rate: {self.sample_rate} Hz")
        print(f"âœ… SNAC layers: {self.num_layers}")

    def process_single_dataset(self, dataset_config: DatasetConfig):
        """Process a single dataset through the pipeline"""
        print(f"\n{'='*60}")
        print(f"ğŸ¯ Processing dataset: {dataset_config.name}")
        print(f"ğŸ“ Dataset prefix: {dataset_config.dataset_prefix}")
        print(f"{'='*60}")

        processor = DatasetProcessor(dataset_config, self.sample_rate)
        processor.load_dataset(num_proc=self.base_settings.load_dataset_num_proc)

        mp.set_start_method("spawn", force=True)
        q = mp.Queue(maxsize=self.base_settings.qsize)

        print(f"\nğŸš€ Starting processing pipeline")
        print(f"ğŸ’» CUDA available: {torch.cuda.is_available()}")
        print(f"ğŸ”¥ GPU workers: {self.num_gpus}")
        print(f"ğŸ“– Reader workers: {self.base_settings.num_readers}")
        print(f"âš™ï¸  Dataset load processes: {self.base_settings.load_dataset_num_proc}")
        print(f"ğŸ“ Output directory: {self.base_settings.OUT_DIR}")
        print(f"ğŸ—‚ï¸  Lines per file: {self.base_settings.lines_per_file:,}")
        print(f"ğŸ“¦ Queue size: {self.base_settings.qsize}")
        print("-" * 60)

        workers = [
            mp.Process(
                target=worker_process,
                args=(
                    i,
                    q,
                    self.base_settings.OUT_DIR,
                    dataset_config.dataset_prefix,
                    self.base_settings.gzip_level,
                    self.base_settings.buffer_size,
                    self.base_settings.lines_per_file,
                    self.base_settings.num_readers,
                    self.base_settings.audio_codec,
                    self.num_layers
                )
            )
            for i in range(self.num_gpus)
        ]

        for p in workers:
            p.start()

        dataset = processor.get_dataset()
        sharded_datasets = [
            dataset.shard(num_shards=self.base_settings.num_readers, index=i)
            for i in range(self.base_settings.num_readers)
        ]

        shard_processors = []
        for i in range(self.base_settings.num_readers):
            shard_proc = DatasetProcessor(dataset_config, self.sample_rate)
            shard_proc.dataset = sharded_datasets[i]
            shard_processors.append(shard_proc)

        readers = [
            mp.Process(
                target=reader_worker_process,
                args=(i, self.base_settings.num_readers, shard_processors[i], q)
            )
            for i in range(self.base_settings.num_readers)
        ]

        for pr in readers:
            pr.start()

        try:
            for pr in readers:
                pr.join()

            for i in range(self.num_gpus):
                q.put(AudioWorker.SENTINEL)

            for p in workers:
                p.join()

            print("\n" + "=" * 60)
            print(f"ğŸ‰ Dataset {dataset_config.name} processed successfully!")

            if os.path.exists(self.base_settings.OUT_DIR):
                files = [f for f in os.listdir(self.base_settings.OUT_DIR)
                        if f.startswith(dataset_config.dataset_prefix)]
                if files:
                    total_size = sum(
                        os.path.getsize(os.path.join(self.base_settings.OUT_DIR, f))
                        for f in files
                    )
                    print(f"ğŸ“Š Generated {len(files)} files for this dataset, "
                          f"size: {total_size / 1024**3:.2f} GB")

        except KeyboardInterrupt:
            print("\nâš ï¸  Interrupted! Terminating processes...")

            for pr in readers:
                pr.terminate()
            for p in workers:
                p.terminate()

            for pr in readers:
                pr.join(timeout=10)
            for p in workers:
                p.join(timeout=10)

            print("ğŸ›‘ All processes terminated")
            raise

    def assemble_and_save_final_dataset(self):
        """Assemble all processed shards into final dataset and save/upload"""
        print(f"\n{'='*60}")
        print("ğŸ”¨ Assembling final dataset from all shards...")
        print(f"{'='*60}")

        shard_files = os.path.join(self.base_settings.OUT_DIR, "*.jsonl.gz")
        print(f"ğŸ“‚ Loading shards from: {shard_files}")

        final_dataset = load_dataset(
            "json",
            data_dir=self.base_settings.OUT_DIR,
            data_files="*.jsonl.gz",
            split='train',
            verification_mode='no_checks'
        )

        print(f"âœ… Final dataset assembled: {len(final_dataset)} samples")

        if self.save_settings.local:
            print(f"\nğŸ’¾ Saving dataset locally to: {self.save_settings.local}")
            final_dataset.save_to_disk(self.save_settings.local)
            print(f"âœ… Dataset saved to disk")

        if self.save_settings.hf_upload:
            print(f"\nâ˜ï¸  Uploading dataset to HuggingFace: {self.save_settings.hf_upload}")
            final_dataset.push_to_hub(self.save_settings.hf_upload, private=True)
            print(f"âœ… Dataset uploaded to HuggingFace Hub")

        print(f"\n{'='*60}")
        print("ğŸŠ Pipeline completed successfully!")
        print(f"{'='*60}")

    def run(self):
        """Run the complete pipeline for all datasets"""
        self.validate()

        datasets = self.config_manager.get_datasets()
        print(f"\nğŸ“‹ Found {len(datasets)} dataset(s) to process")

        for idx, dataset_config in enumerate(datasets, 1):
            print(f"\nğŸ”„ Processing dataset {idx}/{len(datasets)}")
            self.process_single_dataset(dataset_config)

        self.assemble_and_save_final_dataset()

        print("\nğŸ‘‹ Pipeline finished!")
